[environment]
    [environment.settings]
        seed = 44234
        n_env = 256
        dt = 0.05
        record = false

    [environment.scenario]
        n_agents = 1
        n_scripted_entities = 5
        episode_size = 1024
        agent_radius = 40
        ldr_radius = 160
        prb_radius = 50
        ldr_str = 50
        prb_str = 60
        w_c = 0.5
        w_a = 0.7
        w_s = 1.0
        Kp_l = 5.0
        Kp_p = 0.0

    [environment.action_space]
        low = -250
        high = 250
        step = 50

    [environment.observation_space]
        low = 0
        high = 800
[ppo]
    # General parameters
    num_train_steps = 1_000_000
    evaluate_every_epochs = 10_000
    checkpoint_every_epochs = 60_000
    render_every_epochs = 60_000
    num_test_rollouts = 164

    # PPO parameters for learning
    num_train_envs = 256  # "Number of environments for training"
    discount = 0.99 # "Discount factor for rewards"
    gae_lambda = 0.95 # "Lambda for Generalized Advantage Estimation"
    n_steps = 512 # "Number of steps per environment per update"
    n_minibatch = 8 # "Number of PPO minibatches"
    epoch_ppo = 4 # "Number of epochs per PPO update"
    clip_eps = 0.2 # "Clipping parameter for PPO"
    entropy_coeff = 0.01 # "Entropy coefficient for loss calculation" 
    critic_coeff = 0.5 # "Coefficient for critic loss"

    # Optimizer parameters
    lr_begin = 5e-04
    lr_end = 5e-06
    lr_warmup = 0.05 # Prop epochs until warmup is completed 
    max_grad_norm = 1.0
